package main

import (
	"fmt"
	"log"
	"math/rand"
	"os"

	"github.com/xDarkicex/relux"
)

func main() {
	fmt.Println("üöÄ relux ML Framework - Complete Feature Demo")
	fmt.Println("==============================================")

	// ===============================================
	// PART 1: Basic Training & Prediction (XOR)
	// ===============================================
	fmt.Println("\nüìä Part 1: XOR Problem (Non-linear Classification)")

	// XOR dataset - classic non-linearly separable problem
	X := [][]float64{
		{0, 0}, {0, 1}, {1, 0}, {1, 1},
	}
	Y := [][]float64{
		{0}, {1}, {1}, {0},
	}

	// Create network with custom configuration
	net, err := relux.NewNetwork(
		relux.WithConfig(relux.Config{
			Inputs: []relux.InputSpec{{Name: "x", Size: 2}},
			Hidden: []relux.LayerSpec{{Units: 8, Act: "tanh"}},
			Output: relux.LayerSpec{Units: 1, Act: "sigmoid"},
			Loss:   "bce",
		}),
		relux.WithSeed(42), // Deterministic results
	)
	if err != nil {
		log.Fatal("Failed to create network:", err)
	}

	// Display network architecture
	fmt.Printf("Architecture: %s\n", net.Architecture())
	fmt.Printf("Parameters: %d\n", net.ParameterCount())

	// Train the network
	fmt.Println("\nüèãÔ∏è Training...")
	err = net.Fit(X, Y,
		relux.Epochs(5000),
		relux.LearningRate(0.3),
		relux.BatchSize(4),
		relux.Verbose(true),
	)
	if err != nil {
		log.Fatal("Training failed:", err)
	}

	// Test predictions
	fmt.Println("\nüéØ XOR Results:")
	for i, x := range X {
		pred, _ := net.Predict(x)
		fmt.Printf("  Input: %v ‚Üí Expected: %.0f, Got: %.3f ‚úÖ\n",
			x, Y[i][0], pred[0])
	}

	// ===============================================
	// PART 2: Model Persistence
	// ===============================================
	fmt.Println("\nüíæ Part 2: Model Persistence")

	// Save the trained model
	filename := "xor_model.gob"
	err = net.SaveFile(filename)
	if err != nil {
		log.Fatal("Failed to save model:", err)
	}
	fmt.Printf("‚úÖ Model saved to %s\n", filename)

	// Load the model
	loadedNet, err := relux.LoadNetwork(filename)
	if err != nil {
		log.Fatal("Failed to load model:", err)
	}
	fmt.Printf("‚úÖ Model loaded from %s\n", filename)

	// Verify loaded model works
	testInput := []float64{1, 0}
	pred, _ := loadedNet.Predict(testInput)
	fmt.Printf("  Loaded model test: %v ‚Üí %.3f\n", testInput, pred[0])

	// Clean up
	os.Remove(filename)

	// ===============================================
	// PART 3: Configuration Presets
	// ===============================================
	fmt.Println("\n‚öôÔ∏è Part 3: Configuration Presets")

	// Demonstrate different preset configurations
	presets := map[string]relux.Config{
		"Small MLP":      relux.SmallMLP(10, 1),
		"Medium MLP":     relux.MediumMLP(20, 3),
		"Large MLP":      relux.LargeMLP(50, 5),
		"Classification": relux.ClassificationMLP(15, 2, "medium"),
		"Regression":     relux.RegressionMLP(8, 1, "small"),
	}

	for name, config := range presets {
		net, _ := relux.NewNetwork(relux.WithConfig(config))
		fmt.Printf("  %s: %s (%d params)\n",
			name, net.Architecture(), net.ParameterCount())
	}

	// ===============================================
	// PART 4: Batch Prediction
	// ===============================================
	fmt.Println("\nüîÑ Part 4: Batch Operations")

	// Create a regression network
	regNet, _ := relux.NewNetwork(
		relux.WithConfig(relux.RegressionMLP(3, 1, "small")),
		relux.WithSeed(123),
	)

	// Generate synthetic regression data (y = sum of inputs)
	batchX := make([][]float64, 100)
	batchY := make([][]float64, 100)
	for i := 0; i < 100; i++ {
		x1, x2, x3 := rand.Float64(), rand.Float64(), rand.Float64()
		batchX[i] = []float64{x1, x2, x3}
		batchY[i] = []float64{x1 + x2 + x3} // Simple sum
	}

	// Quick training
	fmt.Println("  Training regression model...")
	regNet.Fit(batchX, batchY,
		relux.Epochs(500),
		relux.LearningRate(0.01),
		relux.Verbose(false),
	)

	// Test batch prediction
	testBatch := batchX[:5] // First 5 samples
	predictions, err := regNet.PredictBatch(testBatch)
	if err != nil {
		log.Fatal("Batch prediction failed:", err)
	}

	fmt.Println("  Batch Prediction Results:")
	for i, pred := range predictions {
		expected := batchY[i][0]
		fmt.Printf("    Sample %d: Expected=%.2f, Predicted=%.2f\n",
			i+1, expected, pred[0])
	}

	// Test concurrent batch prediction
	fmt.Println("  Testing concurrent batch prediction...")
	concurrentPreds, err := regNet.PredictBatchConcurrent(testBatch, 2)
	if err != nil {
		log.Fatal("Concurrent batch prediction failed:", err)
	}
	fmt.Printf("  ‚úÖ Concurrent prediction completed (%d results)\n", len(concurrentPreds))

	// ===============================================
	// PART 5: Model Introspection & Debugging
	// ===============================================
	fmt.Println("\nüîç Part 5: Model Introspection")

	// Create a complex network for demonstration
	complexNet, _ := relux.NewNetwork(
		relux.WithConfig(relux.Config{
			Inputs: []relux.InputSpec{{Name: "features", Size: 10}},
			Hidden: []relux.LayerSpec{
				{Units: 64, Act: "relu"},
				{Units: 32, Act: "relu"},
				{Units: 16, Act: "tanh"},
			},
			Output: relux.LayerSpec{Units: 3, Act: "identity"},
			Loss:   "mse",
		}),
	)

	// Display detailed summary
	fmt.Println(complexNet.Summary())

	// Show layer-by-layer information
	fmt.Println("Layer Details:")
	sizes := complexNet.LayerSizes()
	for i, size := range sizes {
		weights, biases, _ := complexNet.GetLayerWeights(i)
		fmt.Printf("  Layer %d: %d units, %dx%d weights, %d biases\n",
			i+1, size, len(weights), len(weights[0]), len(biases))
	}

	// Validate network structure
	err = complexNet.Validate()
	if err != nil {
		fmt.Printf("‚ùå Network validation failed: %v\n", err)
	} else {
		fmt.Println("‚úÖ Network validation passed")
	}

	// ===============================================
	// PART 6: Real-world Classification Example - NOW WITH PHASE 4!
	// ===============================================
	fmt.Println("\nüé≤ Part 6: Iris Classification with Phase 4 Improvements")

	// Same Iris dataset
	irisX := [][]float64{
		{5.1, 3.5, 1.4, 0.2}, // Setosa
		{4.9, 3.0, 1.4, 0.2}, // Setosa
		{7.0, 3.2, 4.7, 1.4}, // Versicolor
		{6.4, 3.2, 4.5, 1.5}, // Versicolor
		{6.3, 3.3, 6.0, 2.5}, // Virginica
		{5.8, 2.7, 5.1, 1.9}, // Virginica
	}
	irisY := [][]float64{
		{1, 0, 0}, // Setosa (one-hot)
		{1, 0, 0}, // Setosa
		{0, 1, 0}, // Versicolor
		{0, 1, 0}, // Versicolor
		{0, 0, 1}, // Virginica
		{0, 0, 1}, // Virginica
	}

	// Create classification network (now with softmax + categorical crossentropy!)
	irisNet, _ := relux.NewNetwork(
		relux.WithConfig(relux.ClassificationMLP(4, 3, "small")),
		relux.WithSeed(456),
	)

	fmt.Printf("Phase 4 Iris Network: %s\n", irisNet.Architecture())
	fmt.Printf("Loss Function: %s (upgraded from MSE!)\n", irisNet.LossName())

	// Train with better settings
	fmt.Println("Training with Phase 4 improvements...")
	irisNet.Fit(irisX, irisY,
		relux.Epochs(1000),
		relux.LearningRate(0.001), // Much smaller LR
		relux.EarlyStopping(50),   // Stop before overfitting
		relux.BatchSize(6),        // Full batch
		relux.Verbose(true),
		relux.LearningRateDecay(0.87, 500),
	)

	// Test classification with detailed probability output
	fmt.Println("Phase 4 Classification Results (with probabilities):")
	classes := []string{"Setosa", "Versicolor", "Virginica"}
	correctCount := 0

	for i, x := range irisX {
		pred, _ := irisNet.Predict(x)

		// Find predicted class
		maxIdx := 0
		for j := 1; j < len(pred); j++ {
			if pred[j] > pred[maxIdx] {
				maxIdx = j
			}
		}

		// Find true class
		trueIdx := 0
		for j := 1; j < len(irisY[i]); j++ {
			if irisY[i][j] > irisY[i][trueIdx] {
				trueIdx = j
			}
		}

		status := "‚úÖ"
		if maxIdx == trueIdx {
			correctCount++
		} else {
			status = "‚ùå"
		}

		// Show full probability distribution (thanks to softmax!)
		fmt.Printf("  Sample %d: True=%s, Predicted=%s [%.3f, %.3f, %.3f] %s\n",
			i+1, classes[trueIdx], classes[maxIdx], pred[0], pred[1], pred[2], status)
	}

	accuracy := float64(correctCount) / float64(len(irisX)) * 100
	fmt.Printf("üéØ Phase 4 Accuracy: %.1f%% (%d/%d) - Major improvement!\n",
		accuracy, correctCount, len(irisX))

	// ===============================================
	// PART 7: Phase 4 Advanced Features Showcase
	// ===============================================
	fmt.Println("\nüî• Part 7: Phase 4 Advanced Features")

	// Demonstrate new activation functions
	fmt.Println("New Activation Functions Available:")
	activationDemos := map[string]relux.Config{
		"GELU (Transformer)": {
			Inputs: []relux.InputSpec{{Name: "x", Size: 4}},
			Hidden: []relux.LayerSpec{{Units: 16, Act: "gelu"}},
			Output: relux.LayerSpec{Units: 2, Act: "softmax"},
			Loss:   "categorical_crossentropy",
		},
		"Swish (Mobile)": {
			Inputs: []relux.InputSpec{{Name: "x", Size: 4}},
			Hidden: []relux.LayerSpec{{Units: 16, Act: "swish"}},
			Output: relux.LayerSpec{Units: 2, Act: "softmax"},
			Loss:   "categorical_crossentropy",
		},
		"LeakyReLU": {
			Inputs: []relux.InputSpec{{Name: "x", Size: 4}},
			Hidden: []relux.LayerSpec{{Units: 16, Act: "leaky_relu"}},
			Output: relux.LayerSpec{Units: 2, Act: "softmax"},
			Loss:   "categorical_crossentropy",
		},
	}

	for name, config := range activationDemos {
		net, _ := relux.NewNetwork(relux.WithConfig(config))
		fmt.Printf("  %s: %s\n", name, net.Architecture())
	}

	fmt.Println("\nPhase 4 Loss Functions:")
	fmt.Println("  ‚úÖ Categorical Cross-Entropy (multi-class)")
	fmt.Println("  ‚úÖ Sparse Categorical Cross-Entropy (integer labels)")
	fmt.Println("  ‚úÖ Binary Cross-Entropy (binary classification)")
	fmt.Println("  ‚úÖ Mean Squared Error (regression)")

	// ===============================================
	// FINAL SUMMARY - PHASE 4 EDITION
	// ===============================================
	fmt.Println("\nüéâ Phase 4 Demo Complete! relux Framework Features:")
	fmt.Println("  ‚úÖ Neural Network Training (Backpropagation + SGD)")
	fmt.Println("  ‚úÖ Advanced Activation Functions (ReLU, Sigmoid, Tanh, Softmax, GELU, Swish, LeakyReLU)")
	fmt.Println("  ‚úÖ Advanced Loss Functions (MSE, BCE, Categorical Cross-Entropy)")
	fmt.Println("  ‚úÖ Model Persistence (Save/Load with gob)")
	fmt.Println("  ‚úÖ Batch Prediction (Sequential & Concurrent)")
	fmt.Println("  ‚úÖ Configuration Presets (Small/Medium/Large MLPs)")
	fmt.Println("  ‚úÖ Model Introspection & Validation")
	fmt.Println("  ‚úÖ Production-Ready Error Handling")
	fmt.Println("  ‚úÖ Go-Idiomatic API Design")
	fmt.Println("  üÜï Proper Multi-Class Classification (Softmax + CrossEntropy)")
	fmt.Println("  üÜï Modern Activation Functions (GELU, Swish)")
	fmt.Println("  üÜï Professional Probability Outputs")
	fmt.Println("\nüöÄ Phase 4: Now truly enterprise-grade for any ML workload!")
}
